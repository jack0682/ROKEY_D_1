import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CameraInfo
from cv_bridge import CvBridge
import cv2
import math
import os
import sys
import threading
import queue
from ultralytics import YOLO
import numpy as np
from rclpy.executors import MultiThreadedExecutor
from geometry_msgs.msg import PointStamped
import tf2_ros
import tf2_geometry_msgs  # ê¼­ í•„ìš”
from datetime import datetime, timezone
#7.16
from rclpy.qos import QoSDurabilityPolicy
from rclpy.qos import QoSHistoryPolicy
from rclpy.qos import QoSProfile
from rclpy.qos import QoSReliabilityPolicy

#7.17
from vision_msgs.msg import BaseCoordinate

# 7.18
# mqtt ìŠ› ~
import json
from paho.mqtt import client as mqtt_client
import time
broker = 'p021f2cb.ala.asia-southeast1.emqxsl.com'
port = 8883
username = 'Rokey'
password = '1234567'
topic = "detect"
client_id = f'yolo_vision'


MARKER_TOPIC = 'detected_objects_marker'
MAP_TOPIC = 'map_points'
BASE_TOPIC = "base_points"

LEN_THRESHOLD = 0.3

qos_profile = QoSProfile(
    reliability=QoSReliabilityPolicy.RELIABLE, 
    history=QoSHistoryPolicy.KEEP_LAST,
    depth=10, 
    durability=QoSDurabilityPolicy.TRANSIENT_LOCAL
    )



# ========================
# ì„¤ì •
# ========================
MODEL_PATH = '/home/jsj2204/Downloads/yolov5n_4classes.pt'
IMAGE_TOPIC = '/robot3/oakd/rgb/preview/image_raw'
DEPTH_TOPIC = '/robot3/oakd/stereo/image_raw'
CAMERA_INFO_TOPIC = '/robot3/oakd/stereo/camera_info'
TARGET_CLASS_ID = 2
NORMALIZE_DEPTH_RANGE = 3.0
WINDOW_NAME = 'YOLO Detection'


# ========================
# ë…¸ë“œ ì •ì˜
# ========================

##########7_21 filtering shoot~############
class OutlierFilter:
    def __init__(self, window_size=5, outlier_threshold=1.0):
        self.window = []
        self.window_size = window_size
        self.outlier_threshold = outlier_threshold  # ë¯¸í„° ë‹¨ìœ„
        
    def update(self, measurement):
        # ìœˆë„ìš°ê°€ ë¹„ì–´ìˆìœ¼ë©´ ë°”ë¡œ ì¶”ê°€
        if len(self.window) == 0:
            self.window.append(measurement)
            return measurement
        
        # í˜„ì¬ í‰ê· ê³¼ì˜ ì°¨ì´ í™•ì¸
        current_avg = np.mean(self.window)
        if abs(measurement - current_avg) > self.outlier_threshold:
            # ì•„ì›ƒë¼ì´ì–´ë©´ ì´ì „ ê°’ ë°˜í™˜
            return current_avg
        
        # ì •ìƒ ê°’ì´ë©´ ìœˆë„ìš°ì— ì¶”ê°€
        self.window.append(measurement)
        if len(self.window) > self.window_size:
            self.window.pop(0)
        
        # ì´ë™í‰ê·  ë°˜í™˜
        return np.mean(self.window)

class SimpleKalmanFilter:
    def __init__(self, process_variance=1e-2, measurement_variance=1e-2):
        # ìƒíƒœ ë³€ìˆ˜
        self.x = 0.0  # ì¶”ì •ëœ ê±°ë¦¬
        self.P = 1.0  # ì¶”ì • ì˜¤ì°¨ì˜ ê³µë¶„ì‚°
        
        # ë…¸ì´ì¦ˆ íŒŒë¼ë¯¸í„° (ë” ê· í˜•ì¡íˆê²Œ ì¡°ì •)
        self.Q = process_variance    # í”„ë¡œì„¸ìŠ¤ ë…¸ì´ì¦ˆ
        self.R = measurement_variance # ì¸¡ì • ë…¸ì´ì¦ˆ
        
        self.is_initialized = False
        self.outlier_filter = OutlierFilter(window_size=3, outlier_threshold=2.0)
    
    def update(self, measurement):
        # ë¨¼ì € ì•„ì›ƒë¼ì´ì–´ í•„í„° ì ìš©
        filtered_measurement = self.outlier_filter.update(measurement)
        
        if not self.is_initialized:
            # ì²« ë²ˆì§¸ ì¸¡ì •ê°’ìœ¼ë¡œ ì´ˆê¸°í™” (ì•„ì›ƒë¼ì´ì–´ í•„í„°ë§ëœ ê°’ ì‚¬ìš©)
            self.x = filtered_measurement
            self.is_initialized = True
            return self.x
        
        # ì˜ˆì¸¡ ë‹¨ê³„
        P_pred = self.P + self.Q
        
        # ì—…ë°ì´íŠ¸ ë‹¨ê³„
        K = P_pred / (P_pred + self.R)
        self.x = self.x + K * (filtered_measurement - self.x)
        self.P = (1 - K) * P_pred
        
        return self.x
###########################################################################


class DetectWithDepthWithTf(Node):
    def __init__(self):
        super().__init__('detect_with_depth_with_tf')
        #7_21
        self.last_coord_len = 0

        # mqtt setup
        # MQTT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.mqtt_client = None
        self.mqtt_connected = False
        self.setup_mqtt()

        self.K = None
        self.should_exit = False
        self.should_shutdown = False

        self.depth_mm = None
        self.depth_colored = None

        self.detect_cx = 0
        self.detect_cy = 0

        self.lock = threading.Lock()  # ê³µìœ  ë³€ìˆ˜ ë³´í˜¸ìš© Lock

        self.bridge = CvBridge()
        self.image_queue = queue.Queue(maxsize=1)

        # ì¹¼ë§Œ í•„í„°
        self.kalman_filter = SimpleKalmanFilter(
            process_variance=1e-2,     # ì¡°ê¸ˆ ë” í¬ê²Œ - ë¹ ë¥¸ ë³€í™” í—ˆìš©
            measurement_variance=1e-2  # í›¨ì”¬ ì‘ê²Œ - ì„¼ì„œë¥¼ ë” ì‹ ë¢°
        )

        # TF Bufferì™€ Listener ì¤€ë¹„
        self.tf_buffer = tf2_ros.Buffer()
        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)

        # 5ì´ˆ í›„ì— ë³€í™˜ ì‹œì‘
        self.get_logger().info("TF Tree ì•ˆì •í™” ì‹œì‘. 5ì´ˆ í›„ ë³€í™˜ ì‹œì‘í•©ë‹ˆë‹¤.")
        self.start_timer = self.create_timer(5.0, self.start_transform)

        # ëª¨ë¸ ë¡œë”©
        if not os.path.exists(MODEL_PATH):
            self.get_logger().error(f"Model not found: {MODEL_PATH}")
            sys.exit(1)

        self.model = YOLO(MODEL_PATH)
        self.class_names = getattr(self.model, 'names', [])

        # êµ¬ë… ì„¤ì •
        self.rgb_img_subscription = self.create_subscription(Image, IMAGE_TOPIC, self.image_callback, 10)
        self.stereo_img_subscription = self.create_subscription(Image, DEPTH_TOPIC, self.depth_callback, 10)
        self.camera_info_subscription = self.create_subscription(CameraInfo, CAMERA_INFO_TOPIC, self.camera_info_callback, 10)

        # 7.16
        self.map_point_pub = self.create_publisher(PointStamped, MAP_TOPIC, qos_profile)

        #7.17
        self.base_coor_pub = self.create_publisher(BaseCoordinate, BASE_TOPIC, qos_profile)

        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘
        self.worker_thread = threading.Thread(target=self.visualization_loop)
        self.worker_thread.daemon = True
        self.worker_thread.start()


    def apply_average_filter(self, depth_image, kernel_size=5):
        """5x5 í‰ê·  í•„í„° ì ìš©"""
        # 0ê°’(ë¬´íš¨í•œ ëìŠ¤)ì€ í•„í„°ë§ì—ì„œ ì œì™¸
        mask = depth_image > 0
        
        # í‰ê·  í•„í„° ì»¤ë„ ìƒì„±
        kernel = np.ones((kernel_size, kernel_size), np.float32) / (kernel_size * kernel_size)
        
        # ìœ íš¨í•œ í”½ì…€ë§Œ í•„í„°ë§
        filtered = cv2.filter2D(depth_image.astype(np.float32), -1, kernel)
        
        # ì›ë³¸ì—ì„œ 0ì´ì—ˆë˜ í”½ì…€ì€ 0ìœ¼ë¡œ ìœ ì§€
        filtered[~mask] = 0
        
        return filtered.astype(depth_image.dtype)

    def camera_info_callback(self, msg):
        if self.K is None:
            self.K = np.array(msg.k).reshape(3, 3)
            self.get_logger().info(f"CameraInfo received: fx={self.K[0,0]:.2f}, fy={self.K[1,1]:.2f}, cx={self.K[0,2]:.2f}, cy={self.K[1,2]:.2f}")

    def depth_callback(self, msg):
        if self.should_exit or self.K is None:
            return

        raw_depth = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')

        # 5x5 í‰ê·  í•„í„° ì ìš©
        depth_mm = self.apply_average_filter(raw_depth, kernel_size=5)

        depth_vis = np.nan_to_num(depth_mm, nan=0.0)
        depth_vis = np.clip(depth_vis, 0, NORMALIZE_DEPTH_RANGE * 1000)
        depth_vis = (depth_vis / (NORMALIZE_DEPTH_RANGE * 1000) * 255).astype(np.uint8)
        depth_colored = cv2.applyColorMap(depth_vis, cv2.COLORMAP_JET)

        with self.lock:
            self.depth_mm = depth_mm
            self.depth_colored = depth_colored

    def image_callback(self, msg):
        frame = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

        if not self.image_queue.full():
            self.image_queue.put(frame)
        else:
            try:
                self.image_queue.get_nowait()
            except queue.Empty:
                pass
            self.image_queue.put(frame)

    def visualization_loop(self):
        while not self.should_shutdown:
            try:
                frame = self.image_queue.get(timeout=0.1)
            except queue.Empty:
                continue

            try:
                results = self.model(frame, stream=True)
            except Exception as e:
                self.get_logger().error(f"YOLO inference error: {e}")
                continue

            with self.lock:
                if self.depth_mm is None or self.depth_colored is None:
                    continue
                depth_mm = self.depth_mm.copy()
                depth_colored = self.depth_colored.copy()

                object_count = 0
                
                # ëª¨ë“  detectionëœ ê°ì²´ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
                all_detections = []
                class_2_boxes = []  # class 2 (person) ë°”ìš´ë”© ë°•ìŠ¤ë“¤
                
                # ì²« ë²ˆì§¸ íŒ¨ìŠ¤: ëª¨ë“  detection ìˆ˜ì§‘
                for r in results:
                    for box in r.boxes:
                        cls = int(box.cls[0])
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        conf = float(box.conf[0])
                        
                        # confidence threshold í™•ì¸
                        if conf < 0.65:
                            continue
                        
                        detection = {
                            'class': cls,
                            'bbox': (x1, y1, x2, y2),
                            'confidence': conf,
                            'center': ((x1 + x2) // 2, (y1 + y2) // 2)
                        }
                        
                        all_detections.append(detection)
                        
                        # class 2 (person) ë°•ìŠ¤ë“¤ ë³„ë„ ì €ì¥
                        if cls == 2:
                            class_2_boxes.append(detection)

                # ë‘ ë²ˆì§¸ íŒ¨ìŠ¤: class 2 ë°•ìŠ¤ì— ëŒ€í•´ ì¡°ê±´ í™•ì¸ ë° ì²˜ë¦¬
                for person_detection in class_2_boxes:
                    person_bbox = person_detection['bbox']
                    person_x1, person_y1, person_x2, person_y2 = person_bbox
                    person_center = person_detection['center']
                    person_conf = person_detection['confidence']
                    
                    # ì´ person ë°•ìŠ¤ ì•ˆì— ìˆëŠ” ë‹¤ë¥¸ í´ë˜ìŠ¤ë“¤ ì°¾ê¸°
                    classes_inside = set()
                    
                    for detection in all_detections:
                        if detection['class'] == 2:  # person ìì²´ëŠ” ì œì™¸
                            continue
                        
                        obj_center_x, obj_center_y = detection['center']
                        
                        # ê°ì²´ì˜ ì¤‘ì‹¬ì ì´ person ë°”ìš´ë”© ë°•ìŠ¤ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
                        if (person_x1 <= obj_center_x <= person_x2 and 
                            person_y1 <= obj_center_y <= person_y2):
                            classes_inside.add(detection['class'])
                            
                            # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸
                            self.get_logger().debug(f"Class {detection['class']} found inside person box at ({obj_center_x}, {obj_center_y})")

                    # ì¡°ê±´ í™•ì¸: class 0, 1, 3 ì¤‘ í•˜ë‚˜ë¼ë„ ì—†ëŠ”ì§€ í™•ì¸
                    required_classes = {0, 1, 3}
                    condition_met = not required_classes.issubset(classes_inside)  # ì¡°ê±´ ë°˜ì „!
                    
                    missing_classes = required_classes - classes_inside
                    self.get_logger().info(f"Person box: classes inside = {classes_inside}, missing = {missing_classes}, condition met = {condition_met}")
                    
                    # ì‹œê°í™” - person ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                    color = (0, 255, 0) if condition_met else (0, 0, 255)  # ì´ˆë¡ìƒ‰ = ì¡°ê±´ ë§Œì¡±(ëˆ„ë½ ìˆìŒ), ë¹¨ê°„ìƒ‰ = ì¡°ê±´ ë¶ˆë§Œì¡±(ëª¨ë‘ ìˆìŒ)
                    thickness = 3 if condition_met else 2
                    cv2.rectangle(frame, (person_x1, person_y1), (person_x2, person_y2), color, thickness)
                    
                    # ì¡°ê±´ì´ ë§Œì¡±ë  ë•Œë§Œ ë©”ì‹œì§€ í¼ë¸”ë¦¬ì‹œ ì§„í–‰
                    if condition_met:
                        detect_cx, detect_cy = person_center
                        
                        # ë²”ìœ„ ì²´í¬
                        if 0 <= detect_cx < depth_mm.shape[1] and 0 <= detect_cy < depth_mm.shape[0]:
                            # depth ê°’ ê°€ì ¸ì˜¤ê¸°
                            distance_mm = depth_mm[detect_cy, detect_cx]
                            distance_m = distance_mm / 1000.0

                            # ì¹¼ë§Œ í•„í„° ì ìš©
                            filtered_distance = self.kalman_filter.update(distance_m)
                            
                            # depth ê°’ì´ ìœ íš¨í•œì§€ í™•ì¸
                            if filtered_distance <= 0 or filtered_distance > NORMALIZE_DEPTH_RANGE:
                                self.get_logger().warn(f"Invalid depth value: {filtered_distance:.2f}m at ({detect_cx}, {detect_cy})")
                                continue
                                
                            self.get_logger().info(f"âœ… CONDITION MET! Person detected with missing objects: {missing_classes}")
                            self.get_logger().info(f"center at (u={detect_cx}, v={detect_cy}) â†’ Distance = {filtered_distance:.2f} meters")

                            # depth ì´ë¯¸ì§€ì— ì¤‘ì‹¬ì  í‘œì‹œ
                            cv2.circle(depth_colored, (detect_cx, detect_cy), 5, (0, 0, 0), -1)
                            
                            # ë ˆì´ë¸” í…ìŠ¤íŠ¸
                            label = self.class_names[2] if 2 < len(self.class_names) else "person"
                            status_text = f"{label}: {person_conf:.2f}, {filtered_distance:.2f}m [MISSING: {missing_classes}]"
                            cv2.putText(frame, status_text, (person_x1, person_y1 - 10),
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
                            
                            # ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°
                            fx = self.K[0,0]
                            fy = self.K[1,1]
                            cx = self.K[0,2]
                            cy = self.K[1,2]
                            
                            # 3D ì¢Œí‘œ ë³€í™˜
                            X = (detect_cx - cx) * filtered_distance / fx
                            Y = (detect_cy - cy) * filtered_distance / fy
                            Z = filtered_distance
                            
                            self.get_logger().info(f"3D coordinates: X={X:.3f}, Y={Y:.3f}, Z={Z:.3f}")

                            try:
                                # base_link ê¸°ì¤€ í¬ì¸íŠ¸ ìƒì„±
                                point_base = PointStamped()
                                point_base.header.stamp = rclpy.time.Time().to_msg()
                                point_base.header.frame_id = 'oakd_link'
                                point_base.point.x = Z  # ì¹´ë©”ë¼ ì¢Œí‘œê³„ì—ì„œ ZëŠ” ì „ë°© ê±°ë¦¬
                                point_base.point.y = X  # ì¹´ë©”ë¼ ì¢Œí‘œê³„ì—ì„œ XëŠ” ì¢Œìš° (ë¶€í˜¸ ì£¼ì˜)
                                point_base.point.z = -Y  # ì¹´ë©”ë¼ ì¢Œí‘œê³„ì—ì„œ YëŠ” ìƒí•˜ (ë¶€í˜¸ ì£¼ì˜)

                                # for end direction
                                point_base_ = PointStamped()
                                point_base_.header.stamp = point_base.header.stamp
                                point_base_.header.frame_id = 'base_link'
                                point_base_.point.x = 0.0
                                point_base_.point.y = 0.0
                                point_base_.point.z = 0.0


                                # base_link â†’ map ë³€í™˜
                                try:
                                    point_map = self.tf_buffer.transform(
                                        point_base,
                                        'map',
                                        timeout=rclpy.duration.Duration(seconds=0.5)
                                    )
                                    self.get_logger().info(f"[oakd_link] ({point_base.point.x:.2f}, {point_base.point.y:.2f}, {point_base.point.z:.2f})")
                                    self.get_logger().info(f"[Map]       ({point_map.point.x:.2f}, {point_map.point.y:.2f}, {point_map.point.z:.2f})")

                                    point_map_ = self.tf_buffer.transform(
                                        point_base_,
                                        'map',
                                        timeout=rclpy.duration.Duration(seconds=0.5)
                                    )
                                    self.get_logger().info(f"[Map_base]       ({point_map_.point.x:.2f}, {point_map.point.y:.2f}, {point_map.point.z:.2f})")

                                    base_vector = [(point_map.point.x - point_map_.point.x), 
                                                   (point_map.point.y - point_map_.point.y)]
                                    unit_base_vector = [(base_vector[0]/((base_vector[0]**2 + base_vector[1]**2))**(1/2)),
                                                        (base_vector[1]/((base_vector[0]**2 + base_vector[1]**2))**(1/2))]
                                    
                                    angle_radians = math.atan2(unit_base_vector[1], unit_base_vector[0])
                                    # angle_degrees = int(math.degrees(angle_radians))
                                    angle_degrees = math.degrees(angle_radians)
                                    
                                    
                                    # ë§µ í¬ì¸íŠ¸ ë°ì´í„° ì¤€ë¹„
                                    point_map_send_data = PointStamped()
                                    point_map_send_data.header.stamp = rclpy.time.Time().to_msg()
                                    point_map_send_data.header.frame_id = 'map'
                                    point_map_send_data.point.x = point_map.point.x
                                    point_map_send_data.point.y = point_map.point.y
                                    # point_map_send_data.point.z = point_map.point.z
                                    point_map_send_data.point.z = angle_degrees

                                    base_msg = BaseCoordinate()
                                    base_msg.x = point_base.point.x
                                    base_msg.y = point_base.point.y
                                    self.base_coor_pub.publish(base_msg)

                                    # # MQTT ë©”ì‹œì§€ë„ ì¡°ê±´ì´ ë§Œì¡±ë  ë•Œë§Œ ì „ì†¡
                                    current_lenght = point_base.point.x + point_base.point.y
                                    if abs(current_lenght - self.last_coord_len) <  LEN_THRESHOLD:
                                        self.publish_mqtt_message(point_map, distance_m)
                                        
                                        self.get_logger().info("ğŸ“¤ Messages published successfully!")
                                    self.last_coord_len = current_lenght
                                except Exception as e:
                                    self.get_logger().warn(f"TF transform to map failed: {e}")

                            except Exception as e:
                                self.get_logger().warn(f"Unexpected error: {e}")
                    else:
                        # ì¡°ê±´ì´ ë§Œì¡±ë˜ì§€ ì•Šì„ ë•Œ (ëª¨ë“  ê°ì²´ê°€ ë‹¤ ìˆì„ ë•Œ)
                        label = self.class_names[2] if 2 < len(self.class_names) else "person"
                        status_text = f"{label}: {person_conf:.2f} [ALL OBJECTS PRESENT - NOT PUBLISHING]"
                        cv2.putText(frame, status_text, (person_x1, person_y1 - 10),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

                    object_count += 1

                # ë‹¤ë¥¸ í´ë˜ìŠ¤ë“¤ë„ ì‹œê°í™” (ì°¸ê³ ìš©)
                for detection in all_detections:
                    if detection['class'] != 2:  # personì´ ì•„ë‹Œ ê°ì²´ë“¤
                        cls = detection['class']
                        x1, y1, x2, y2 = detection['bbox']
                        conf = detection['confidence']
                        label = self.class_names[cls] if cls < len(self.class_names) else f"class_{cls}"
                        
                        # ì‘ì€ ë°”ìš´ë”© ë°•ìŠ¤ë¡œ í‘œì‹œ (íŒŒë€ìƒ‰)
                        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 1)
                        cv2.putText(frame, f"{label}: {conf:.2f}", (x1, y1 - 5),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)

            cv2.putText(frame, f"Persons: {len(class_2_boxes)}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

            display_img = cv2.resize(frame, (frame.shape[1] * 2, frame.shape[0] * 2))
            cv2.imshow(WINDOW_NAME, display_img)

            key = cv2.waitKey(1)
            if key == ord('q'):
                self.should_shutdown = True
                self.get_logger().info("Q pressed. Shutting down...")
                break

    def start_transform(self):
        self.get_logger().info("TF Tree ì•ˆì •í™” ì™„ë£Œ. ë³€í™˜ ì‹œì‘í•©ë‹ˆë‹¤.")

        # ì£¼ê¸°ì  ë³€í™˜ íƒ€ì´ë¨¸ ë“±ë¡
        # self.transform_timer = self.create_timer(2.0, self.timer_callback)

        # ì‹œì‘ íƒ€ì´ë¨¸ ì¤‘ì§€ (í•œ ë²ˆë§Œ ì‹¤í–‰)
        self.start_timer.cancel()

##############mqtt############################
    def setup_mqtt(self):
        """MQTT í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        def on_connect(client, userdata, flags, rc):
            if rc == 0:
                self.mqtt_connected = True
                print("âœ… Connected to MQTT Broker!")
                self.get_logger().info("MQTT Connected successfully!")
            else:
                self.mqtt_connected = False
                print(f"âŒ Failed to connect to MQTT, return code {rc}")
                self.get_logger().error(f"MQTT Connection failed with code {rc}")

        def on_disconnect(client, userdata, rc):
            self.mqtt_connected = False
            print(f"ğŸ”Œ Disconnected from MQTT Broker with code {rc}")
            self.get_logger().warn(f"MQTT Disconnected with code {rc}")

        def on_publish(client, userdata, mid):
            print(f"ğŸ“¤ Message {mid} published successfully")

        try:
            self.get_logger().info("ğŸ”„ Attempting to connect to MQTT broker...")
            self.mqtt_client = mqtt_client.Client(client_id=client_id, protocol=mqtt_client.MQTTv311)
            self.mqtt_client.tls_set()
            self.mqtt_client.username_pw_set(username, password)
            self.mqtt_client.on_connect = on_connect
            self.mqtt_client.on_disconnect = on_disconnect
            self.mqtt_client.on_publish = on_publish
            
            # ë¹„ë™ê¸° ì—°ê²° ì‹œì‘
            self.mqtt_client.connect_async(broker, port, keepalive=60)
            self.mqtt_client.loop_start()
            
            # ì—°ê²° í™•ì¸ì„ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
            time.sleep(2)
            
        except Exception as e:
            self.get_logger().error(f"MQTT setup error: {e}")
            self.mqtt_client = None


    def publish_mqtt_message(self, map_point, distance_m):
        """MQTT ë©”ì‹œì§€ ë°œí–‰"""
        if not self.mqtt_client or not self.mqtt_connected:
            self.get_logger().warn("MQTT not connected. Skipping message.")
            return
            
        try:
            # í˜„ì¬ ì‹œê°„ì„ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì‚¬ìš©
            stamp = time.time()
            
            msg = {
                "robot_id": "robot3",
                "type": "human3",
                "location": [round(map_point.point.x, 2), round(map_point.point.y, 2)],
                "depth": round(distance_m, 2),
                "area": 0,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
            # msg_json = json.dumps(msg, indent=2)
            msg_json = json.dumps(msg)
            
            # ë©”ì‹œì§€ ë°œí–‰
            result = self.mqtt_client.publish(topic, msg_json)
            
            if result.rc == mqtt_client.MQTT_ERR_SUCCESS:
                print(f"ğŸ“¨ Sent message to topic `{topic}`")
                self.get_logger().info(f"MQTT message sent: {msg}")
            else:
                print(f"âŒ Failed to send message to topic {topic}, error: {result.rc}")
                self.get_logger().error(f"MQTT publish failed with code {result.rc}")
                
        except Exception as e:
            self.get_logger().error(f"MQTT publish error: {e}")
##############################################


# ========================
# ë©”ì¸ í•¨ìˆ˜
# ========================
def main():
    rclpy.init()
    node = DetectWithDepthWithTf()
    executor = MultiThreadedExecutor()
    executor.add_node(node)

    try:
        while rclpy.ok() and not node.should_shutdown:
            executor.spin_once(timeout_sec=0.1)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()
        cv2.destroyAllWindows()
        print("Shutdown complete.")
        sys.exit(0)

if __name__ == '__main__':
    main()